# 📦 Artifacts Directory

This directory contains all the outputs and artifacts generated by the LEMON EEG CNN pipeline.

## 📁 Directory Structure

```
artifacts/
├── models/          # Trained model checkpoints
├── results/         # Evaluation results and analysis
├── logs/           # Training logs and outputs
└── README.md       # This file
```

## 🧠 Models Directory (`models/`)

Contains trained model checkpoints and weights:

- **`best_model.pth`**: The best performing model based on validation accuracy
  - PyTorch state dictionary format
  - Contains CNN weights and biases
  - Used for inference and evaluation

### Model Information
- **Architecture**: 5-layer CNN for EEG classification
- **Input**: 62-channel EEG spectral features
- **Output**: Binary classification (Internet Addicted vs Control)
- **Format**: PyTorch `.pth` file

## 📊 Results Directory (`results/`)

Contains evaluation results and analysis:

- **`evaluation_results.pkl`**: Comprehensive evaluation metrics
  - Cross-validation results
  - Statistical significance tests
  - Performance analysis (confusion matrix, ROC curves)
  - Model interpretability analysis
  - Channel importance rankings

### Results Contents
```python
{
    'cross_validation': {
        'accuracy_mean': float,
        'accuracy_std': float,
        'precision_mean': float,
        'recall_mean': float,
        'f1_mean': float
    },
    'statistical_significance': {
        'p_value': float,
        'confidence_interval': [float, float]
    },
    'performance_analysis': {
        'metrics': dict,
        'confusion_matrix': array,
        'roc_curve': dict,
        'probability_distribution': array
    },
    'interpretability': {
        'channel_importance': array,
        'top_channels': array
    },
    'model_info': {
        'parameters': int,
        'architecture': str
    }
}
```

## 📝 Logs Directory (`logs/`)

Contains training logs and outputs:

- **Training logs**: Detailed training progress
- **Error logs**: Debugging information
- **Performance logs**: Runtime metrics

## 🔄 Workflow

1. **Training** (`scripts/train.py`):
   - Saves best model to `artifacts/models/best_model.pth`

2. **Evaluation** (`scripts/evaluate.py`):
   - Loads model from `artifacts/models/best_model.pth`
   - Saves results to `artifacts/results/evaluation_results.pkl`

3. **Analysis**:
   - Load results from `artifacts/results/evaluation_results.pkl`
   - Generate reports and visualizations

## 🧹 Maintenance

### Cleanup
To clean old artifacts:
```bash
# Remove all artifacts
rm -rf artifacts/*

# Remove only models
rm -rf artifacts/models/*

# Remove only results
rm -rf artifacts/results/*
```

### Backup
Important artifacts should be backed up:
```bash
# Backup models
cp -r artifacts/models/ backup/models/

# Backup results
cp -r artifacts/results/ backup/results/
```

## 📋 Version Control

- **`.gitignore`**: Artifacts are typically excluded from version control
- **Large files**: Model files can be large (~8MB)
- **Reproducibility**: Use W&B for experiment tracking instead

## 🎯 Best Practices

1. **Naming**: Use descriptive names with timestamps
2. **Versioning**: Keep multiple versions of important models
3. **Documentation**: Document model performance and parameters
4. **Cleanup**: Regularly remove old artifacts to save space
5. **Backup**: Backup important results and models

---

**Note**: This directory is automatically created by the training and evaluation scripts. No manual setup required. 